# Defines Driver Configuration

# Loads the following (first-listed are higher priority):
# system properties
# application.conf (all resources on classpath with this name)
# application.json (all resources on classpath with this name)
# application.properties (all resources on classpath with this name)
# reference.conf (all resources on classpath with this name)
# This is the reference config file that contains all the default settings.

# Make your edits/overrides in your application.conf.

# ${?property.name} allows the application driver to override using a system property or environmnet variable
# property will only be set if a value exists

# driver specific properties
driver {
  # driver application name
  appName = defaultAppName
  appName = ${?appName}
  # driver alert threshold
  alert.threshold = 0
}

#spark specific properties
spark {
  # spark master url, default to empty string, spark will override property while deploying driver
  master = ""
  master = ${?master}
  # location of the spark checkpoint dir, needs to be overriden
  sql.streaming.checkpointLocation = override
  sql.streaming.checkpointLocation = ${?sql.streaming.checkpointLocation}
  # number of shuffle partition per query
  sql.shuffle.partition = 100
  sql.shuffle.partition = ${?sql.shuffle.partition}
  # mode associated to the output sink, defaults to complete
  output.mode = complete
  output.mode =  ${?output.mode}
  # keeps track of how long to keep data, defaults to 60 minutes
  watermark.time = 60 minutes
  watermark.time = ${?watermark.time}
  # window duration on processing time, if needed, defaults to 60 minutes
  window.duration = 60 minutes
  window.duration = ${?window.duration}
  # sliding window duration, if needed, default to the window duration, if window.duration == window.slide.duration
  # then window is defined as a tumbling window, i.e. fixed window
  window.slide.duration = 60 minutes
  window.slide.duration = ${?window.slide.duration}
  # trigger interval (say, every 1 second), new rows get appended to the input table, default of 0 indicates, retrieving
  # new data as soon as proceesing the previous data is finished
  query.trigger = 0 seconds
  query.trigger = ${?query.trigger}
  # address of the cassandra server host, needs to be overriden
  cassandra.connection.host = localhost
  cassandra.connection.host = ${?cassandra.connection.host}
  # address of the cassandra server port, needs to be overriden
  cassandra.connection.port = 9042
  cassandra.connection.port = ${?cassandra.connection.port}

  # authentication username, needs to be overriden
  cassandra.auth.username = none
  cassandra.auth.username = ${?username}
  # authentication password, needs to be overriden
  cassandra.auth.password = none
  cassandra.auth.password = ${?password}
}

# kafka spark specific properties
kafka {
  # ip address and port of the kafka bootstrap, needs to be overriden
  bootstrap.servers = "localhost:9999"
  bootstrap.servers = ${?bootstrap.servers}
  # list of kafka topics, will fail if not set
  topics = ${?topics}
  # kafka output source
  output.topics = default
  output.topics = ${?output.topics}
  # the topic list to subscribe. Only one of "assign", "subscribe" or "subscribePattern" options can be specified
  # for Kafka source
  subscribe = subscribe
  subscribe = ${?subscribe}
  # upon startup, where to read in the stream
  startingOffsets = earliest
  startingOffsets = ${?startingOffsets}
  # Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will
  # be proportionally split across topicPartitions of different volume.
  maxOffsetsPerTrigger = 50000
  maxOffsetsPerTrigger = ${?maxOffsetsPerTrigger}
}

# cassadra specific properties
cassandra {
  # cassandra keyspace
  keyspace = override
  keyspace = ${?keyspace}
}